% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/process_data.R
\name{run_slurm}
\alias{run_slurm}
\title{Run geoBAM in parallel using batchtools where each reach is submitted
as a job.}
\usage{
run_slurm(
  reaches,
  input_dir,
  output_dir,
  partition,
  max_jobs,
  as_job_array = FALSE
)
}
\arguments{
\item{reaches}{list of string reach identifiers}

\item{input_dir}{string input directory}

\item{output_dir}{string output directory}

\item{partition}{string name of partition to run on}

\item{max_jobs}{maximum number of jobs to be run concurrently; enter -1 to disable chunking}

\item{as_job_array}{boolean indicates if job should be run as a job array}
}
\description{
NOTE: To AVOID submitting too many jobs to the cluster you need to consider
the number of reaches (jobs) that you will need to process in parallel and
the max number of cores you have available to you. Each execution of geobamdata
takes 4 cores per reach. So if you divide the max number of cores by 4 you
should get the max_jobs you can run concurrently and that is what you need
to pass to the max_jobs parameter.
}
